# Language-Modeling
This repo serves as an introduction to language modeling and the various models and methods, developed for the same. 
We start with a simple frequency-based `N-gram Model` and its variants and move all the way upto `LSTM Models` while making a comparison
of the perplexity given by these models.

## Model Perplexity
`N-gram`: 504.405 (Bigram)<br>
`N-gram with discounted backoff`: 271.095 (Trigram)<br>
`Neural N-gram`: 264.960 (Trigram)<br>
`LSTM Sequential`: 157.973 (3 Layers)<br>
