{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9wJrwPoyWjl"
      },
      "outputs": [],
      "source": [
        "!pip install torchtext==0.6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROOC1HBfviMi"
      },
      "source": [
        "##Loading Libraries and Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtWBU5pewb-t",
        "outputId": "b4c253dd-fe8a-407b-faaa-47bee40e7e3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<eos>', '=', 'Homarus', 'gammarus', '=', '<eos>', '<eos>', 'Homarus', 'gammarus', ',', 'known', 'as', 'the', 'European', 'lobster', 'or', 'common', 'lobster', ',', 'is', 'a', 'species', 'of', '<unk>', 'lobster', 'from', 'the', 'eastern', 'Atlantic', 'Ocean']\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import math\n",
        "import tqdm\n",
        "import random\n",
        "import pdb\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "\n",
        "# download and load the data\n",
        "text_field = torchtext.data.Field()\n",
        "datasets = torchtext.datasets.WikiText2.splits(root='/content/drive/MyDrive/NLP/Language_Modeling', text_field=text_field)\n",
        "train_dataset, validation_dataset, test_dataset = datasets\n",
        "\n",
        "text_field.build_vocab(train_dataset, validation_dataset, test_dataset)\n",
        "vocab = text_field.vocab\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "train_text = train_dataset.examples[0].text # a list of tokens (strings)\n",
        "validation_text = validation_dataset.examples[0].text\n",
        "\n",
        "print(validation_text[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcZ6-Dtuvpwl"
      },
      "source": [
        "##Unigram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WkI9HzX0PJe"
      },
      "outputs": [],
      "source": [
        "class UnigramModel:\n",
        "  def __init__(self, train_text):\n",
        "    self.counts = Counter(train_text)\n",
        "    self.total_count = len(train_text)\n",
        "\n",
        "  def probability(self, word):\n",
        "    return self.counts[word] / self.total_count\n",
        "\n",
        "  def next_word_probabilities(self, text_prefix):\n",
        "    return [self.probability(word) for word in vocab.itos]\n",
        "\n",
        "  def perplexity(self, full_text):\n",
        "    log_probs = []\n",
        "    for word in full_text:\n",
        "      log_probs.append(math.log(self.probability(word),2))\n",
        "\n",
        "    return 2 ** -np.mean(log_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH3b7Ke-tyko",
        "outputId": "78be5e39-f1a7-49dd-9e1e-ba048b818440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigram validation perplexity: 965.0860734119312\n"
          ]
        }
      ],
      "source": [
        "unigram_demonstration_model = UnigramModel(train_text)\n",
        "print('unigram validation perplexity:',\n",
        "      unigram_demonstration_model.perplexity(validation_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXFLnwDAxTZz"
      },
      "outputs": [],
      "source": [
        "def check_validity(model):\n",
        "    \"\"\"Performs several sanity checks on your model:\n",
        "    1) That next_word_probabilities returns a valid distribution\n",
        "    2) That perplexity matches a perplexity calculated from next_word_probabilities\n",
        "\n",
        "    Although it is possible to calculate perplexity from next_word_probabilities,\n",
        "    it is still good to have a separate more efficient method that only computes\n",
        "    the probabilities of observed words.\n",
        "    \"\"\"\n",
        "\n",
        "    log_probabilities = []\n",
        "    for i in range(10):\n",
        "        prefix = validation_text[:i]\n",
        "        probs = model.next_word_probabilities(prefix)\n",
        "        assert min(probs) >= 0, \"Negative value in next_word_probabilities\"\n",
        "        assert max(probs) <= 1 + 1e-8, \"Value larger than 1 in next_word_probabilities\"\n",
        "        assert abs(sum(probs)-1) < 1e-4, \"next_word_probabilities do not sum to 1\"\n",
        "\n",
        "        word_id = vocab.stoi[validation_text[i]]\n",
        "        selected_prob = probs[word_id]\n",
        "        log_probabilities.append(math.log(selected_prob))\n",
        "\n",
        "    perplexity = math.exp(-np.mean(log_probabilities))\n",
        "    your_perplexity = model.perplexity(validation_text[:10])\n",
        "    assert abs(perplexity-your_perplexity) < 0.1, \"your perplexity does not \" + \\\n",
        "    \"match the one we calculated from `next_word_probabilities`,\\n\" + \\\n",
        "    \"at least one of `perplexity` or `next_word_probabilities` is incorrect.\\n\" + \\\n",
        "    f\"we calcuated {perplexity} from `next_word_probabilities`,\\n\" + \\\n",
        "    f\"but your perplexity function returned {your_perplexity} (on a small sample).\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxS_v0PIxlmc"
      },
      "outputs": [],
      "source": [
        "check_validity(unigram_demonstration_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ih4QfEiWt3Rb"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, n=20, prefix=('<eos>', '<eos>')):\n",
        "  prefix = list(prefix)\n",
        "  for _ in range(n):\n",
        "    probs = model.next_word_probabilities(prefix)\n",
        "    word = random.choices(vocab.itos, probs)[0]\n",
        "    prefix.append(word)\n",
        "\n",
        "  return ' '.join(prefix)\n",
        "\n",
        "\n",
        "print(generate_text(unigram_demonstration_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peOSthDsvsuZ"
      },
      "source": [
        "## N-gram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B6jwJapQB02"
      },
      "outputs": [],
      "source": [
        "def n_grams(text, n):\n",
        "    n_gram = []\n",
        "\n",
        "    for i in range(len(text)-(n-1)):\n",
        "      words = []\n",
        "      j = i\n",
        "      while(j<=len(text) and j-i+1 <= n):\n",
        "        words.extend([text[j]])\n",
        "        j+=1\n",
        "      n_gram.append(tuple(words))\n",
        "\n",
        "    return n_gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaKJlc8HuA0x"
      },
      "outputs": [],
      "source": [
        "class NGramModel:\n",
        "  def __init__(self, train_text, n=2, alpha = 3e-3):\n",
        "    self.n = n\n",
        "    self.smoothing = alpha\n",
        "    self.counts_n_gram = Counter(n_grams(train_text, n))\n",
        "    self.counts_n_gram_1 = Counter(n_grams(train_text, n-1))\n",
        "\n",
        "\n",
        "  def n_gram_probability(self, n_gram):\n",
        "    assert len(n_gram) == self.n\n",
        "    if(self.n>1):\n",
        "        return (self.counts_n_gram[tuple(n_gram)] + self.smoothing) / (self.counts_n_gram_1[tuple(n_gram[:len(n_gram)-1])] + (vocab_size * self.smoothing))\n",
        "    else:\n",
        "        return (self.counts_n_gram[tuple(n_gram)] + self.smoothing) / (len(train_text) + (vocab_size * self.smoothing))\n",
        "\n",
        "  def next_word_probabilities(self, text_prefix):\n",
        "    probs = []\n",
        "    for word in vocab.itos:\n",
        "      if(len(text_prefix)<self.n-1):\n",
        "         probs.append(1/vocab_size)\n",
        "      else:\n",
        "        n_gram = text_prefix[(len(text_prefix) - self.n+1):len(text_prefix)] + [word]\n",
        "        probs.append(self.n_gram_probability(n_gram))\n",
        "\n",
        "    return probs\n",
        "\n",
        "  def perplexity(self, full_text):\n",
        "    log_probs = []\n",
        "    for i in range(len(full_text)):\n",
        "      if(i>=self.n-1):\n",
        "        n_gram = full_text[i-self.n+1:i+1]\n",
        "        log_probs.append(math.log(self.n_gram_probability(n_gram), 2))\n",
        "      else:\n",
        "        log_probs.append(1/vocab_size)\n",
        "    return 2 ** -np.mean(log_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNwGUMW34hWz",
        "outputId": "9c89e74b-5d61-4628-a49e-23625c1ee0c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigram validation perplexity: 965.0913686618096\n",
            "bigram validation perplexity: 504.4054886536929\n",
            "trigram validation perplexity: 2965.381793306292\n"
          ]
        }
      ],
      "source": [
        "unigram_model = NGramModel(train_text, 1)\n",
        "print('unigram validation perplexity:', unigram_model.perplexity(validation_text)) # this should be the almost the same as our unigram model perplexity above\n",
        "\n",
        "bigram_model = NGramModel(train_text, n=2)\n",
        "print('bigram validation perplexity:', bigram_model.perplexity(validation_text))\n",
        "\n",
        "trigram_model = NGramModel(train_text, n=3)\n",
        "print('trigram validation perplexity:', trigram_model.perplexity(validation_text)) # this won't do very well..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dsyPz4VA8W9h",
        "outputId": "018714aa-5301-4512-96cb-ebcd1f3a32f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<eos> <eos> <unk> 2003 Medical A usually way sent possible the school into the . grew Echmarcach toll had Benares 's tenure\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "generate_text(unigram_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4lSmQ_hHHocR",
        "outputId": "cac91bd4-7000-41f0-cf13-2c9d96d8a27f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<eos> <eos> Joe 's nose , and Tennyson pyramids wonderful Piedras TA 1751 joins Dentists Graves Tomb outgassing Hancock fledglings anarchy NC\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "generate_text(bigram_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PIDM7IkYHrmC",
        "outputId": "7cf10eb2-5c52-4a78-abc4-e8d6c72a2d83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<eos> <eos> = = Career = mm unjust paradoxical Although Baibars Wynne indefinitely discretion margins Performing Nadu O. motion suggests 1754 Armstrong'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "generate_text(trigram_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rb7o5ccbfQUY"
      },
      "outputs": [],
      "source": [
        "# Free up some RAM.\n",
        "del bigram_model\n",
        "del trigram_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discounted Backoff"
      ],
      "metadata": {
        "id": "mkA-3JVfOcCc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsXEikd_4nZ0"
      },
      "source": [
        "This basic model works okay for bigrams, but a better strategy (especially for higher-order models) is to use backoff.  Implement backoff with absolute discounting.\n",
        "$$P\\left(w_i|w_{i-n+1}^{i-1}\\right)=\\frac{max\\left\\{C(w_{i-n+1}^i)-\\delta,0\\right\\}}{\\sum_{w_i} C(w_{i-n+1}^i)} + \\alpha(w_{i-n+1}^{i-1}) P(w_i|w_{i-n+2}^{i-1})$$\n",
        "\n",
        "$$\\alpha\\left(w_{i-n+1}^{i-1}\\right)=\\frac{\\delta N_{1+}(w_{i-n+1}^{i-1})}{{\\sum_{w_i} C(w_{i-n+1}^i)}}$$\n",
        "where $N_{1+}$ is the number of words that appear after the previous $n-1$ words (the number of times the max will select something other than 0 in the first equation).  If $\\sum_{w_i} C(w_{i-n+1}^i)=0$, use the lower order model probability directly (the above equations would have a division by 0).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-82xM2UYgAhL"
      },
      "outputs": [],
      "source": [
        "class DiscountBackoffModel(NGramModel):\n",
        "  def __init__(self, train_text, lower_order_model, n=2, delta=0.9):\n",
        "    super().__init__(train_text, n = n)\n",
        "    self.lower_order_model = lower_order_model\n",
        "    self.discount = delta\n",
        "    self.counts_n_gram = Counter(n_grams(train_text,n))\n",
        "    self.counts_n_gram_1 = Counter(n_grams(train_text,n-1))\n",
        "\n",
        "  def n_gram_probability(self, n_gram):\n",
        "    assert len(n_gram) == self.n\n",
        "    n_gram_1 = n_gram[:len(n_gram)-1]\n",
        "    total_count_n_gram = 0\n",
        "    N = 0\n",
        "    for word in vocab.itos:\n",
        "      text = n_gram_1 + [word]\n",
        "      total_count_n_gram += self.counts_n_gram[tuple(text)]\n",
        "      N += min(self.counts_n_gram[tuple(text)],1)\n",
        "\n",
        "    if(total_count_n_gram == 0):\n",
        "      return self.lower_order_model.n_gram_probability(n_gram[1:])\n",
        "\n",
        "    prob = max(self.counts_n_gram[tuple(n_gram)] - self.discount, 0) / total_count_n_gram\n",
        "    alpha = (self.discount * N) / total_count_n_gram\n",
        "    prob += alpha * self.lower_order_model.n_gram_probability(n_gram[1:])\n",
        "    return prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oJ9Ef7r14wK-",
        "outputId": "7845c80d-95d9-4427-e2ef-d5c5049e0504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trigram backoff validation perplexity: 271.0957323219511\n"
          ]
        }
      ],
      "source": [
        "bigram_backoff_model = DiscountBackoffModel(train_text, unigram_model, 2)\n",
        "trigram_backoff_model = DiscountBackoffModel(train_text, bigram_backoff_model, 3)\n",
        "print('trigram backoff validation perplexity:', trigram_backoff_model.perplexity(validation_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xme8E9uQSe-"
      },
      "outputs": [],
      "source": [
        "del unigram_model\n",
        "del bigram_backoff_model\n",
        "del trigram_backoff_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural N-gram Model"
      ],
      "metadata": {
        "id": "PhLTkM1t8ORG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ids(tokens):\n",
        "  return [vocab.stoi[t] for t in tokens]\n",
        "\n",
        "assert torch.cuda.is_available()\n",
        "\n",
        "class NeuralNgramDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, text_token_ids, n):\n",
        "    self.text_token_ids = text_token_ids\n",
        "    self.n = n\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text_token_ids)\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    if i < self.n-1:\n",
        "      prev_token_ids = [vocab.stoi['<eos>']] * (self.n-i-1) + self.text_token_ids[:i]\n",
        "    else:\n",
        "      prev_token_ids = self.text_token_ids[i-self.n+1:i]\n",
        "\n",
        "    assert len(prev_token_ids) == self.n-1\n",
        "\n",
        "    x = torch.tensor(prev_token_ids)\n",
        "    y = torch.tensor(self.text_token_ids[i])\n",
        "    return x, y\n",
        "\n",
        "class NeuralNGramNetwork(nn.Module):\n",
        "  def __init__(self,n):\n",
        "    super().__init__()\n",
        "    self.n = n\n",
        "    self.net = nn.Sequential(\n",
        "    nn.Linear((n-1)*128,1024),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.1),\n",
        "    nn.Linear(1024,128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.1),\n",
        "    )\n",
        "    self.final_layer = nn.Linear(128, vocab_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    embeds = F.embedding(x,self.final_layer.weight)\n",
        "    if(len(embeds)>2):\n",
        "      embeds = torch.flatten(embeds,1)\n",
        "    else:\n",
        "      embeds = torch.flatten(embeds)\n",
        "    out = self.net(embeds)\n",
        "    out = F.log_softmax(self.final_layer(out))\n",
        "    return out\n",
        "\n",
        "\n",
        "class NeuralNGramModel:\n",
        "  def __init__(self,n):\n",
        "    self.n = n\n",
        "    self.network = NeuralNGramNetwork(n).cuda()\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    self.optimizer = torch.optim.Adam(self.network.parameters(),1e-3)\n",
        "\n",
        "  def train(self):\n",
        "    dataset = NeuralNgramDataset(ids(train_text), self.n)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset, batch_size =128, shuffle=True)\n",
        "    for epoch in range(10):\n",
        "      print('epoch: {}'.format(epoch + 1))\n",
        "      running_loss = 0\n",
        "      for prefix, target in (train_loader):\n",
        "        prefix = prefix.cuda()\n",
        "        target = target.cuda()\n",
        "        output = self.network(prefix)\n",
        "        loss = self.criterion(output,target)\n",
        "        running_loss += loss\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "      print('Loss: {:.6f}'.format(running_loss/len(train_loader)))\n",
        "\n",
        "  def next_word_probabilities(self, text_prefix):\n",
        "    self.network.eval()\n",
        "    probs = []\n",
        "    prefix = text_prefix + [\"<eos>\"]\n",
        "    dataset = NeuralNgramDataset(ids(prefix), self.n)\n",
        "    prefix, target = dataset[len(prefix)-1]\n",
        "    prefix = prefix.cuda()\n",
        "    output = self.network(prefix)\n",
        "    for i in range(vocab_size):\n",
        "      probs.append(math.exp(output[i]))\n",
        "    return probs\n",
        "\n",
        "  def perplexity(self,text):\n",
        "    log_probs = []\n",
        "    self.network.eval()\n",
        "    dataset = NeuralNgramDataset(ids(text), self.n)\n",
        "    for i in range(len(text)):\n",
        "       prefix, target = dataset[i]\n",
        "       prefix = prefix.cuda()\n",
        "       target = target.cuda()\n",
        "       output = self.network(prefix)\n",
        "       log_probs.append(math.log(math.exp(output[target]),2))\n",
        "    return 2 ** -np.mean(log_probs)"
      ],
      "metadata": {
        "id": "9Kmf-CaK8REJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neural_trigram_model = NeuralNGramModel(3)\n",
        "check_validity(neural_trigram_model)\n",
        "neural_trigram_model.train()"
      ],
      "metadata": {
        "id": "FVQ-9r61vNaB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdfeeb44-89de-46dc-b140-7b9f52c6046a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-791f7dc09e3b>:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = F.log_softmax(self.final_layer(out))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1\n",
            "Loss: 5.919364\n",
            "epoch: 2\n",
            "Loss: 5.387251\n",
            "epoch: 3\n",
            "Loss: 5.195302\n",
            "epoch: 4\n",
            "Loss: 5.075787\n",
            "epoch: 5\n",
            "Loss: 4.991659\n",
            "epoch: 6\n",
            "Loss: 4.926989\n",
            "epoch: 7\n",
            "Loss: 4.877513\n",
            "epoch: 8\n",
            "Loss: 4.836218\n",
            "epoch: 9\n",
            "Loss: 4.803491\n",
            "epoch: 10\n",
            "Loss: 4.776093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('neural trigram validation perplexity:', neural_trigram_model.perplexity(validation_text))"
      ],
      "metadata": {
        "id": "MpkVp3uPFX80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5694175-6dc6-423f-8f76-8f37ee7800b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-791f7dc09e3b>:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = F.log_softmax(self.final_layer(out))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neural trigram validation perplexity: 264.9605479065314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model we don't need.\n",
        "del neural_trigram_model"
      ],
      "metadata": {
        "id": "DrRNzym4kugS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LSTM Language Model"
      ],
      "metadata": {
        "id": "GRPs_cTiUeF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMNetwork (nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.lstm = nn.LSTM(input_size = 128, hidden_size = 512, num_layers = 3, dropout = 0.5)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.linear_1 = nn.Linear(512, 128)\n",
        "    self.linear_2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "  def forward(self, x, state):\n",
        "    embeds = F.embedding(x,self.linear_2.weight)\n",
        "    out, hidden = self.lstm(embeds, state)\n",
        "    out = self.dropout(out)\n",
        "    out = self.linear_1(out)\n",
        "    out = F.log_softmax(self.linear_2(out))\n",
        "    return out, hidden\n",
        "\n",
        "class LSTMModel:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.network = LSTMNetwork().cuda()\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    self.optimizer = torch.optim.Adam(self.network.parameters(),1e-3)\n",
        "\n",
        "  def detach_hidden(self, hidden):\n",
        "    hidden, cell = hidden\n",
        "    hidden = hidden.detach()\n",
        "    cell = cell.detach()\n",
        "    return hidden, cell\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    hidden = torch.zeros(3, batch_size, 512).cuda()\n",
        "    cell = torch.zeros(3, batch_size, 512).cuda()\n",
        "    return hidden, cell\n",
        "\n",
        "  def train(self):\n",
        "    self.network.train()\n",
        "    train_iterator = torchtext.data.BPTTIterator(train_dataset, batch_size = 64,\n",
        "                                                 bptt_len=32, device = 'cuda')\n",
        "    for epoch in range(20):\n",
        "      print('epoch: {}'.format(epoch+1))\n",
        "      running_loss = 0\n",
        "      hidden = self.init_hidden(batch_size = 64)\n",
        "      for batch in train_iterator:\n",
        "        prefix = batch.text.cuda()\n",
        "        target = batch.target.cuda()\n",
        "\n",
        "        self.network.zero_grad()\n",
        "\n",
        "        hidden = self.detach_hidden(hidden)\n",
        "        output, hidden = self.network(prefix, hidden)\n",
        "\n",
        "        loss = self.criterion(output.view(output.size(0)*output.size(1), output.size(2)), target.view(-1))\n",
        "        running_loss += loss\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "      print('Loss: {:.6f}'.format(running_loss/len(train_iterator)))\n",
        "\n",
        "\n",
        "  def next_word_probabilities(self, text_prefix):\n",
        "    prefix_token_tensor = torch.tensor(ids(text_prefix), device='cuda').view(-1, 1)\n",
        "    self.network.eval()\n",
        "    probs = []\n",
        "    hidden = self.init_hidden()\n",
        "    output, hidden = self.network(prefix_token_tensor, hidden)\n",
        "    for i in range(vocab_size):\n",
        "      probs.append(math.exp(output[i]))\n",
        "    return probs\n",
        "\n",
        "  def dataset_perplexity(self, torchtext_dataset):\n",
        "    with torch.no_grad():\n",
        "      iterator = torchtext.data.BPTTIterator(torchtext_dataset, batch_size = 32, bptt_len=32, device='cuda')\n",
        "      losses = []\n",
        "      self.network.eval()\n",
        "      hidden = self.init_hidden(batch_size = 32)\n",
        "      for batch in iterator:\n",
        "        prefix = batch.text.cuda()\n",
        "        target = batch.target.cuda()\n",
        "        output, hidden = self.network(prefix, hidden)\n",
        "        loss = F.cross_entropy(output.view(output.size(0)*output.size(1), output.size(2)), target.view(-1))\n",
        "        losses.append(loss.cpu().numpy())\n",
        "      return math.exp(np.mean(losses))"
      ],
      "metadata": {
        "id": "genWD2GIUf1v"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = LSTMModel()\n",
        "lstm_model.train()"
      ],
      "metadata": {
        "id": "j71HIAGS8WzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ad67b7-c3e2-4903-c8e7-97c539008234"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-a2bb63f53f8f>:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = F.log_softmax(self.linear_2(out))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 7.138511\n",
            "epoch: 2\n",
            "Loss: 6.111068\n",
            "epoch: 3\n",
            "Loss: 5.860744\n",
            "epoch: 4\n",
            "Loss: 5.619581\n",
            "epoch: 5\n",
            "Loss: 5.464509\n",
            "epoch: 6\n",
            "Loss: 5.347710\n",
            "epoch: 7\n",
            "Loss: 5.240488\n",
            "epoch: 8\n",
            "Loss: 5.152400\n",
            "epoch: 9\n",
            "Loss: 5.089320\n",
            "epoch: 10\n",
            "Loss: 5.018318\n",
            "epoch: 11\n",
            "Loss: 4.955593\n",
            "epoch: 12\n",
            "Loss: 4.904957\n",
            "epoch: 13\n",
            "Loss: 4.861936\n",
            "epoch: 14\n",
            "Loss: 4.818425\n",
            "epoch: 15\n",
            "Loss: 4.776217\n",
            "epoch: 16\n",
            "Loss: 4.750140\n",
            "epoch: 17\n",
            "Loss: 4.743425\n",
            "epoch: 18\n",
            "Loss: 4.689441\n",
            "epoch: 19\n",
            "Loss: 4.682596\n",
            "epoch: 20\n",
            "Loss: 4.643999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('lstm validation perplexity:', lstm_model.dataset_perplexity(validation_dataset))"
      ],
      "metadata": {
        "id": "TLrNr_qs4bfO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6acefb5-1b9a-4c7d-e1cf-df7198d75f23"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-a2bb63f53f8f>:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = F.log_softmax(self.linear_2(out))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lstm validation perplexity: 157.97293265712904\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}